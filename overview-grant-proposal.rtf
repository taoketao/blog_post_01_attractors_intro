{\rtf1\ansi\deff3\adeflang1025
{\fonttbl{\f0\froman\fprq2\fcharset0 Times New Roman;}{\f1\froman\fprq2\fcharset2 Symbol;}{\f2\fswiss\fprq2\fcharset0 Arial;}{\f3\froman\fprq2\fcharset128 Times New Roman;}{\f4\fswiss\fprq2\fcharset128 Arial;}{\f5\fnil\fprq2\fcharset128 SimSun;}{\f6\fnil\fprq2\fcharset128 Lucida Sans;}{\f7\fswiss\fprq0\fcharset128 Lucida Sans;}}
{\colortbl;\red0\green0\blue0;\red128\green128\blue128;}
{\stylesheet{\s0\snext0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033 Default;}
{\s15\sbasedon0\snext16\sb240\sa120\keepn\hich\af5\dbch\af6\afs28\loch\f4\fs28 Heading;}
{\s16\sbasedon0\snext16\sb0\sa120 Text body;}
{\s17\sbasedon16\snext17\sb0\sa120\dbch\af7 List;}
{\s18\sbasedon0\snext18\sb120\sa120\noline\i\dbch\af7\afs24\ai\fs24 Caption;}
{\s19\sbasedon0\snext19\noline\dbch\af7 Index;}
}{\info{\author Morgan Bryant}{\creatim\yr2021\mo9\dy7\hr2\min31}{\author Morgan Bryant}{\revtim\yr2021\mo9\dy7\hr3\min42}{\printim\yr0\mo0\dy0\hr0\min0}{\comment OpenOffice}{\vern4000}}\deftab709

{\*\pgdsctbl
{\pgdsc0\pgdscuse195\pgwsxn12240\pghsxn15840\marglsxn1134\margrsxn1134\margtsxn1134\margbsxn1134\pgdscnxt0 Default;}}
\formshade\paperh15840\paperw12240\margl1134\margr1134\margt1134\margb1134\sectd\sbknone\sectunlocked1\pgndec\pgwsxn12240\pghsxn15840\marglsxn1134\margrsxn1134\margtsxn1134\margbsxn1134\ftnbj\ftnstart1\ftnrstcont\ftnnar\aenddoc\aftnrstcont\aftnstart1\aftnnrlc
\pgndec\pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch
    }{\rtlch \ltrch\loch
I'm interested in matrices with multiple eigenvectors of [maximal] eigenvalue 1. Understanding this question better }
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch
    }{\rtlch \ltrch\loch
First, some background on stochastic matrices. Any real stochastic matrices (Markov chain processes) with (contingently) a spectral radius of 1 having a }{\i\ai\rtlch \ltrch\loch
single}{\rtlch \ltrch\loch
 eigenvector of eigenvalue 1 (ie and where all other eigenvectors have eigenvalues less than 1 in absolute value), then repeated application of the matrix operator is guaranteed to converge to that eigenvector as the unique stationary lowest-energy distribution state from any initial state. When there are more than one eigenvector sharing eigenvalue 1, each of these stationary distributions are (equally??) likely, stationary, stable, and lowest-energy, and the one converged to depends on the initial distribution; [[ recursive macro on convexity of stationaries? ]]. (However, a stochastic matrix with any amount of randomness will almost surely not have its eigenvalue 1 have multiplicity greater than 1.)}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch
    }{\rtlch \ltrch\loch
In linear neural networks, training* adjusts the feedforward matrix layer towards being a matrix of orthonormal eigenvectors*. Specifically, the network tends to focus its training improvements on the single strongest mode until it's learned fully and then proceeding to optimize for the next mode, up until the matrix is saturated into a full-rank suite of eigenvectors. (This mode-by-mode behavior concords with the Low-Rank Approximation via Eckart\uc3 \u8211\'e2\'80\'93Young\u8211\'e2\'80\'93Mirsky theorem.) Therefore, the number of stationary optimal distributions tends to increase.\uc1 }
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch
    }{\rtlch \ltrch\loch
Attractors, as they are called in dynamical systems, are observed in biological neural network (such as in grid and place cells*) and in idealized/theoretical recurrent models such as the Hopfield network. From work in dynamical systems, it's well known that attractors are located at eigenvectors. This is no coincidence: it is a consequence of 'Markovity / ergodicity' of such a system. However, it is also well known that in grid and place cell ensembles, that states can be held stationary in one of }{\i\ai\rtlch \ltrch\loch
many}{\rtlch \ltrch\loch
 attractors. For example, head direction cells implement ring attractors, a collection of attractors that are individually state-stationary but permit states can move to adjacent states if the creature moves (and integrates) its head direction angle. The presence of many attractors accords with the expectation that these an adult brain's neuron ensembles for fundamental motion and sense of location and direction are more or less fully-trained.}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch
    }{\rtlch \ltrch\loch
The open questions lie in understanding multiple attractors. What are their shapes? (This is somewhat solved. I'm still digesting the results.) What are their interplay? (This is likely solved/resolved, and I've yet to locate the results.) What does it take to move a state from one attractor to the next? (A relatively easy problem amounting to orthogonal vector arithmetic for some kinds of matrices. Some results: such transitions must be symmetric ,by maybe 'detailed reversibility' M_ij p_i = M_ji p_j?; MCMC methods or temperature-based simulated annealing procedure would be available for experiments; ...) And, what I'm most curious about, how do the answers to these questions shed light on the plausibility of neurons implementing a virtual state-machine automaton? If these attractor machines are implemented by parallel distributed neural networks and cursorily resemble the activity of neurons themselves* (and would therefore be recursive), do they implement }{\i\ai\rtlch \ltrch\loch
distributed}{\i0\ai0\rtlch \ltrch\loch
 state machines, and what are the properties of such parallel distributed automata?}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\i0\ai0\rtlch \ltrch
    }{\i0\ai0\rtlch \ltrch\loch
Other questions of related but tangential curiosity and would benefit first from lit review research: Do distributed attractor systems have a fundamental distinction between states and actions between the states? Neuron knowledge is somewhat inextricable from activity.}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\i0\ai0\rtlch \ltrch
    }{\i0\ai0\rtlch \ltrch\loch
I propose:}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\i0\ai0\rtlch \ltrch\loch
(a) examining the eigenspectra of different kinds of random neural networks, such as those defined by stochastic matrices, matrices those with restricted values such as ternary (a la Hopfield networks), binary (for which there seems to be substantial work done especially in spectral graph theory), nearly-orthogonal, network layers along training course, etc. I've started running simulations of the eigenspectra of such matrices in <https://github.com/taoketao/random_matrices_and_eigenvalues>. In accordance with the literature, the distributions of eigenvalues of many matrices follow the Marchenko-Pastur distribution, which is characterized by y=sqrt(1/x-1). I hope to develop some insight into quasi-eigenvalue 'stationary distributions' of proper neural networks layers adorned with nonlinear activation functions.}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\i0\ai0\rtlch \ltrch\loch
(b) finish reviewing and digest the existing literature on results and answers to the questions I've asked. There's plenty to learn from specific papers and from the general study of dynamical systems, thermodynamics, mathematical treatments of Hopfield networks / Boltzmann machines, spin glasses, linear algebra, and high-dimensional applied statistics and optimization (i.e. 'machine learning'), and particularly the work Saxe, Ganguli, Lake, Smolensky.}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
.}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
* 'training': rigor and specification pending. See Saxe, McClelland, and Ganguli, 2013 and 2019. And, generally speaking, while these results have been suggested to be similar for nonlinear layers (pers. comm. PDP lab group ~2017 by jlmcc, Stephen Hansen, Andrew Lampinen), this kind of analysis has not to my knowledge been widely applied to recurrent network layers. Furthermore, the connection between gradual acquisition of orthonormal eigenvectors which correspond to attractors is not }{\i\ai\rtlch \ltrch\loch
tied}{\rtlch \ltrch\loch
 to the question of point, the behavior of multiple viable attractors.}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
* 'orthonormal eigenvectors': if the orthogonal eigenvectors are not normalized, proper normalization can be achieved by, for example, competitive learning paradigms embraced in theoretical neuroscience, rendering this a minor concern if any. Source: Andrew Lampinen, pers. comm. Rigor pending.}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
* 'grid and place cells': while it is relatively common knowledge that these work by means of attractors, the statement could use direction to evidence and support}
\par \pard\plain \s0\nowidctlpar{\*\hyphen2\hyphlead2\hyphtrail2\hyphmax0}\cf0\kerning1\hich\af5\langfe2052\dbch\af6\afs24\lang1081\loch\f3\fs24\lang1033{\rtlch \ltrch\loch
* 'resemble...': A conjecture! But intuitively plausible: neurons, which hold (fleeting) activation states accessible to many other external processes -- as do attractors -- }{\i\ai\rtlch \ltrch\loch
and}{\rtlch \ltrch\loch
 are modified by inputs with specific linear-hyperplane thresholds akin to the (linear-hyperplane?) edge of two attractor basins, do parallel distributed computing. Based on these characteristics, it would seem that a recurrent layer of neurons could itself implement a virtual recurrent layer of neurons \uc3 \u8211\'e2\'80\'93 albeit, about 0.138 times as large as the base. The question lies in the consistency (~systematicity) of activations between neurons and state transitions, the vertical interactions between recursive levels of hypothetical virtual neural network layers\uc1 }
\par }